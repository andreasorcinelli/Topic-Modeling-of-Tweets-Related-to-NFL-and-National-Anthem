{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading in the aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/Users/andreacsorcinelli/Documents/01_Personal/02_Job Stuff/01_Metis/02_Metis_GH/03_Projects/04_Fletcher/01_Notebooks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agregating data\n",
    "agg_data = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith('.pkl'):\n",
    "        with open(file , 'rb') as picklefile:\n",
    "            ind_data = pickle.load(picklefile)\n",
    "            agg_data.append(ind_data)\n",
    "\n",
    "df = pd.concat(agg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392487, 4)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@1BR4HlM How can i forget that anthem man ðŸ˜‚ \n",
      "\n",
      "RT @PettyNegr0: Mary J. Blige did not go through her divoreces, give us Not Gon Cry, Just Fine, Family Affair, and No More Drama to be disrâ€¦ \n",
      "\n",
      "RT @Luvnediting: @KrisBordessa @DesignationSix @FoxNews @Purina @Starbucks @SuperBetaPro @PetSmart @NFL @GoldBondInc @Terminix @sleepnumberâ€¦ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in df['tweet'][:3]:\n",
    "    print(tweet, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392487"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     43.0\n",
       "1    140.0\n",
       "2    140.0\n",
       "3    140.0\n",
       "4    140.0\n",
       "Name: tweet, dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_lengths = df.tweet.str.len()\n",
    "tweet_lengths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    object\n",
       "_id           object\n",
       "datetime      object\n",
       "tweet         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 392487 entries, 0 to 10207\n",
      "Data columns (total 4 columns):\n",
      "Unnamed: 0    392487 non-null object\n",
      "_id           392476 non-null object\n",
      "datetime      392476 non-null object\n",
      "tweet         392476 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 15.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('null', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5b75de5782816f1bcaa7aaba</td>\n",
       "      <td>2018-08-16 20:28:07</td>\n",
       "      <td>@1BR4HlM How can i forget that anthem man ðŸ˜‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5b75de5882816f1bcaa7aabb</td>\n",
       "      <td>2018-08-16 20:28:07</td>\n",
       "      <td>RT @PettyNegr0: Mary J. Blige did not go throu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5b75de5882816f1bcaa7aabc</td>\n",
       "      <td>2018-08-16 20:28:08</td>\n",
       "      <td>RT @Luvnediting: @KrisBordessa @DesignationSix...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5b75de5882816f1bcaa7aabd</td>\n",
       "      <td>2018-08-16 20:28:08</td>\n",
       "      <td>RT @mflynnJR: My thoughts are we shouldnâ€™t tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5b75de5882816f1bcaa7aabe</td>\n",
       "      <td>2018-08-16 20:28:08</td>\n",
       "      <td>RT @DesignationSix: I have some @FoxNews spons...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                       _id             datetime  \\\n",
       "0          0  5b75de5782816f1bcaa7aaba  2018-08-16 20:28:07   \n",
       "1          1  5b75de5882816f1bcaa7aabb  2018-08-16 20:28:07   \n",
       "2          2  5b75de5882816f1bcaa7aabc  2018-08-16 20:28:08   \n",
       "3          3  5b75de5882816f1bcaa7aabd  2018-08-16 20:28:08   \n",
       "4          4  5b75de5882816f1bcaa7aabe  2018-08-16 20:28:08   \n",
       "\n",
       "                                               tweet  \n",
       "0        @1BR4HlM How can i forget that anthem man ðŸ˜‚  \n",
       "1  RT @PettyNegr0: Mary J. Blige did not go throu...  \n",
       "2  RT @Luvnediting: @KrisBordessa @DesignationSix...  \n",
       "3  RT @mflynnJR: My thoughts are we shouldnâ€™t tak...  \n",
       "4  RT @DesignationSix: I have some @FoxNews spons...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 392487 entries, 0 to 10207\n",
      "Data columns (total 4 columns):\n",
      "Unnamed: 0    392487 non-null object\n",
      "_id           392487 non-null object\n",
      "datetime      392487 non-null object\n",
      "tweet         392487 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 15.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5b75de5782816f1bcaa7aaba</td>\n",
       "      <td>2018-08-16 20:28:07</td>\n",
       "      <td>How can i forget that anthem man ðŸ˜‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5b75de5882816f1bcaa7aabb</td>\n",
       "      <td>2018-08-16 20:28:07</td>\n",
       "      <td>RT  Mary J. Blige did not go through her divor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5b75de5882816f1bcaa7aabc</td>\n",
       "      <td>2018-08-16 20:28:08</td>\n",
       "      <td>RT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5b75de5882816f1bcaa7aabd</td>\n",
       "      <td>2018-08-16 20:28:08</td>\n",
       "      <td>RT  My thoughts are we shouldnâ€™t take any poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5b75de5882816f1bcaa7aabe</td>\n",
       "      <td>2018-08-16 20:28:08</td>\n",
       "      <td>RT  I have some  sponsors tagged here,so they ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                       _id             datetime  \\\n",
       "0          0  5b75de5782816f1bcaa7aaba  2018-08-16 20:28:07   \n",
       "1          1  5b75de5882816f1bcaa7aabb  2018-08-16 20:28:07   \n",
       "2          2  5b75de5882816f1bcaa7aabc  2018-08-16 20:28:08   \n",
       "3          3  5b75de5882816f1bcaa7aabd  2018-08-16 20:28:08   \n",
       "4          4  5b75de5882816f1bcaa7aabe  2018-08-16 20:28:08   \n",
       "\n",
       "                                               tweet  \n",
       "0                 How can i forget that anthem man ðŸ˜‚  \n",
       "1  RT  Mary J. Blige did not go through her divor...  \n",
       "2                                     RT              \n",
       "3  RT  My thoughts are we shouldnâ€™t take any poli...  \n",
       "4  RT  I have some  sponsors tagged here,so they ...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing user mentions\n",
    "df['tweet'].replace(r\"(?:\\@|https?\\://)\\S+\",'', regex=True, inplace=True)\n",
    "#print (df['tweet'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   How can i forget that anthem man ðŸ˜‚\n",
       "1    RT  Mary J Blige did not go through her divore...\n",
       "2                                       RT            \n",
       "3    RT  My thoughts are we shouldnâ€™t take any poli...\n",
       "4    RT  I have some  sponsors tagged hereso they w...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new pandas series that stores all of the tweets with punctuation removed \n",
    "\n",
    "remove_punct = str.maketrans('', '', string.punctuation)\n",
    "tweets_clean_1 = df['tweet'].str.translate(remove_punct)\n",
    "tweets_clean_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the tweets to lower case and remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   how can i forget that anthem man ðŸ˜‚\n",
       "1    rt  mary j blige did not go through her divore...\n",
       "2                                       rt            \n",
       "3    rt  my thoughts are we shouldnâ€™t take any poli...\n",
       "4    rt  i have some  sponsors tagged hereso they w...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_clean_2 = tweets_clean_1.str.lower()\n",
    "\n",
    "remove_digits = str.maketrans('', '', string.digits)\n",
    "tweets_clean_2 = tweets_clean_2.str.translate(remove_digits)\n",
    "tweets_clean_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   how can i forget that anthem man ðŸ˜‚\n",
       "1    rt  mary j blige did not go through her divore...\n",
       "2                                       rt            \n",
       "3    rt  my thoughts are we shouldnâ€™t take any poli...\n",
       "4    rt  i have some  sponsors tagged hereso they w...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove links\n",
    "tweets_clean_3 = tweets_clean_2.str.replace('https','')\n",
    "tweets_clean_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   how can i forget that anthem man ðŸ˜‚\n",
       "1      mary j blige did not go through her divorece...\n",
       "2                                                     \n",
       "3      my thoughts are we shouldnâ€™t take any politi...\n",
       "4      i have some  sponsors tagged hereso they wil...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove retweet\n",
    "tweets_clean_4 = tweets_clean_3.str.replace('rt','')\n",
    "tweets_clean_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize or stem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF & LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreacsorcinelli/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "playe explains nowthisnews american brilliantly orourke ted taking think man\n",
      "Topic 1:\n",
      "denlesks idiots cop tcodqquziinup idea doesn sign team week nfl\n",
      "Topic 2:\n",
      "jasonoverstreet violent bit minutes little best video ve watch just\n",
      "Topic 3:\n",
      "theme drake singlehandedly imsadbro raise josh amp written racist overplayed\n",
      "Topic 4:\n",
      "nfl preseason game new days kickoff players amp la rule\n",
      "Topic 5:\n",
      "meet theellenshow tcorzxakbhw betoorourke like thank looks thoughtful leadership stevekerr\n",
      "Topic 6:\n",
      "stop wish player morningmoneyben lazy protested saying people national anthem\n",
      "Topic 7:\n",
      "example political minute leader divisive conviction address steveschmidtses issue pe\n",
      "Topic 8:\n",
      "time understand politician consider taken jemelehill cho assess sounds like\n",
      "Topic 9:\n",
      "espn football night monday air dont breaking apparently want expose\n",
      "Topic 10:\n",
      "cruz texas rourke govhowarddean outclasses votes deserve guy beto ted\n",
      "Topic 11:\n",
      "catchy fucking baby shark bbiss written racist shit overplayed way\n",
      "Topic 12:\n",
      "know trying didn friends sell months okay sold charlesmblow just\n",
      "Topic 13:\n",
      "victor retirement announces official brnfl tcoozteonbj years cruz old nfl\n",
      "Topic 14:\n",
      "knee players asks agrees muslimiq frustrated offensi taking says betoorourke\n",
      "Topic 15:\n",
      "season kofieyeboah cancel tcoouvuspsp nfl regular stas watching added hall\n",
      "Topic 16:\n",
      "vote perfectly aiculated tcofugbraakl officialjld betoorourke thank thoughtful honest watch\n",
      "Topic 17:\n",
      "anthem national rise kneeling stans taehyung stand yoonseokers taeguyoongi tcoldatvpjwt\n",
      "Topic 18:\n",
      "years barstoolspos prison lacesoutshow guy sentenced tcoekgnsuyuh just hasn heat\n",
      "Topic 19:\n",
      "redskins peterson rb rapsheet adrian mikegarafolo adrianpeterson signing tcovxbqioaac tcowmtagiw\n",
      "Topic 0:\n",
      "nowthisnews nfl players rare player right perfectly football kingjames disrespectful\n",
      "Topic 1:\n",
      "jasonoverstreet violent time address politician years today dont retirement wars\n",
      "Topic 2:\n",
      "old national way written racist anthem shit overplayed catchy bbiss\n",
      "Topic 3:\n",
      "nfl cho assess victor year mlb nba athielen talking actually\n",
      "Topic 4:\n",
      "little amp black love left women immigrants nathanlerner literally playing\n",
      "Topic 5:\n",
      "orourke like sounds answer sold honest agrees hit foxnews wow\n",
      "Topic 6:\n",
      "beto fuckin rourke know doesn idiots denlesks tcodqquziinup minute josh\n",
      "Topic 7:\n",
      "anthem national song sign issue understand consider raise flag drop\n",
      "Topic 8:\n",
      "man betoorourke bit example integrity conviction steveschmidtses thank aiculated officialjld\n",
      "Topic 9:\n",
      "american watch nfl preseason stand game america live hollywood agree\n",
      "Topic 10:\n",
      "explains nfl minutes team week guy idea theme okay won\n",
      "Topic 11:\n",
      "nfl listen votes fan good war going kuwarner elect draft\n",
      "Topic 12:\n",
      "ve jemelehill asks nfl words check tcoeyrbqgciz play la talk\n",
      "Topic 13:\n",
      "meet theellenshow tcorzxakbhw nfl new trying rule muslimiq frustrated offensi\n",
      "Topic 14:\n",
      "best nfl did thoughtful said months tedcruz day watching great\n",
      "Topic 15:\n",
      "cruz ted nfl cop people vote leader didn want charlesmblow\n",
      "Topic 16:\n",
      "texas knee deserve govhowarddean outclasses believe clear sta league bakarisellers\n",
      "Topic 17:\n",
      "brilliantly playe nfl season political divisive friends perfect say free\n",
      "Topic 18:\n",
      "taking video fucking shark kneeling nfl pe sell th hear\n",
      "Topic 19:\n",
      "think just baby drake world head days salute remember hea\n"
     ]
    }
   ],
   "source": [
    "# 20 topics\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "documents = tweets_clean_4\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english',token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\",)\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreacsorcinelli/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "playe explains nowthisnews american brilliantly orourke ted taking cruz think\n",
      "Topic 1:\n",
      "denlesks idiots cop tcodqquziinup idea doesn sign team week nfl\n",
      "Topic 2:\n",
      "jasonoverstreet violent bit minutes little best video ve just watch\n",
      "Topic 3:\n",
      "written racist overplayed shit old way fuckin song national anthem\n",
      "Topic 4:\n",
      "nfl season players years preseason new game victor days kofieyeboah\n",
      "Topic 5:\n",
      "meet theellenshow tcorzxakbhw betoorourke like vote perfectly officialjld tcofugbraakl aiculated\n",
      "Topic 6:\n",
      "stop wish morningmoneyben player protested lazy saying people national anthem\n",
      "Topic 7:\n",
      "example political minute leader divisive conviction address steveschmidtses issue pe\n",
      "Topic 8:\n",
      "time players understand politician consider taken jemelehill cho assess sounds\n",
      "Topic 9:\n",
      "anthem national espn football monday night air want rise dont\n",
      "Topic 0:\n",
      "nfl bit kneeling integrity minute jemelehill listen votes thoughtful football\n",
      "Topic 1:\n",
      "explains playe just nfl ve jasonoverstreet violent time people season\n",
      "Topic 2:\n",
      "ted anthem national old way written racist shit overplayed catchy\n",
      "Topic 3:\n",
      "best cop denlesks tcodqquziinup leader nfl year sta dividing brilliant\n",
      "Topic 4:\n",
      "like minutes theellenshow tcorzxakbhw know sounds nfl answer pe did\n",
      "Topic 5:\n",
      "man think taking video fuckin texas fucking knee sign rare\n",
      "Topic 6:\n",
      "beto cruz orourke brilliantly nowthisnews nfl betoorourke shark rourke vote\n",
      "Topic 7:\n",
      "american watch song little meet nfl issue deserve didn check\n",
      "Topic 8:\n",
      "players amp baby nfl political example address divisive conviction steveschmidtses\n",
      "Topic 9:\n",
      "nfl team week guy idea doesn idiots josh okay sold\n"
     ]
    }
   ],
   "source": [
    "# try 10 topics\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "documents = tweets_clean_4\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english',token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\",)\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 10\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "playe explains brilliantly american orourke\n",
      "Topic 1:\n",
      "cop idea idiots doesn sign\n",
      "Topic 2:\n",
      "like violent bit minutes little\n",
      "Topic 3:\n",
      "anthem national written racist overplayed\n",
      "Topic 4:\n",
      "salute words candid thoughtful watch\n"
     ]
    }
   ],
   "source": [
    "# NMF 5 topics\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "documents = tweets_clean_4\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english',token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\",)\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 5\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "no_top_words = 5\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreacsorcinelli/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "nfl man cruz ted think\n",
      "Topic 1:\n",
      "anthem national thoughtful words written\n",
      "Topic 2:\n",
      "like players nfl watch just\n",
      "Topic 3:\n",
      "beto taking nfl knee texas\n",
      "Topic 4:\n",
      "watch nfl candid salute vote\n"
     ]
    }
   ],
   "source": [
    "# LDA with 5 topics\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "documents = tweets_clean_4\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 5\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 5\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "playe explains nowthisnews brilliantly american\n",
      "Topic 1:\n",
      "denlesks cop idiots tcodqquziinup idea\n",
      "Topic 2:\n",
      "jasonoverstreet violent bit minutes little\n",
      "Topic 3:\n",
      "anthem national written racist overplayed\n",
      "Topic 4:\n",
      "betoorourke salute kingjames words tcoeyrbqgciz\n"
     ]
    }
   ],
   "source": [
    "# NMF 5 topics; max_df = .50\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "documents = tweets_clean_4\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.50, min_df=2, max_features=no_features, stop_words='english',token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\",)\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 5\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "no_top_words = 5\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreacsorcinelli/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "beto cruz ted orourke brilliantly\n",
      "Topic 1:\n",
      "watch just thoughtful words candid\n",
      "Topic 2:\n",
      "man taking explains betoorourke trump\n",
      "Topic 3:\n",
      "like players brown jim week\n",
      "Topic 4:\n",
      "think american nowthisnews anthem national\n"
     ]
    }
   ],
   "source": [
    "# LDA with 5 topics; max_df = .50\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "documents = tweets_clean_4\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.50, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 5\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 5\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "playe explains brilliantly american orourke\n",
      "Topic 1:\n",
      "cop idea idiots doesn sign\n",
      "Topic 2:\n",
      "like violent bit minutes little\n",
      "Topic 3:\n",
      "anthem national written racist overplayed\n",
      "Topic 4:\n",
      "salute words candid thoughtful watch\n"
     ]
    }
   ],
   "source": [
    "# NMF 5 topics; max_df = .75\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "documents = tweets_clean_4\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.75, min_df=2, max_features=no_features, stop_words='english',token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\",)\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 5\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "no_top_words = 5\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "playe explains brilliantly american orourke\n",
      "Topic 1:\n",
      "cop idea idiots sign doesn\n"
     ]
    }
   ],
   "source": [
    "# NMF 5 topics; max_df = .75\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "documents = tweets_clean_4\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.75, min_df=2, max_features=no_features, stop_words='english',token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\",)\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 2\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "no_top_words = 5\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "playe explains brilliantly american orourke ted taking\n",
      "Topic 1:\n",
      "cop idea idiots sign doesn team week\n"
     ]
    }
   ],
   "source": [
    "# NMF 2 topics; max_df = .75\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "documents = tweets_clean_4\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.75, min_df=2, max_features=no_features, stop_words='english',token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\",)\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 2\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "no_top_words = 7\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "playe explains brilliantly american taking orourke cruz\n",
      "Topic 1:\n",
      "cop idea idiots sign doesn team week\n",
      "Topic 2:\n",
      "like watch violent bit minutes little video\n",
      "Topic 3:\n",
      "anthem national written racist overplayed old shit\n"
     ]
    }
   ],
   "source": [
    "# NMF 4 topics; max_df = .75\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "documents = tweets_clean_4\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.75, min_df=2, max_features=no_features, stop_words='english',token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\",)\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 4\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "no_top_words = 7\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreacsorcinelli/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "beto nfl man cruz ted orourke think\n",
      "Topic 1:\n",
      "anthem national written old racist shit overplayed\n",
      "Topic 2:\n",
      "nfl players words football candid salute vote\n",
      "Topic 3:\n",
      "watch nfl like just best video ve\n"
     ]
    }
   ],
   "source": [
    "# LDA with 4 topics; max_df = .75\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "documents = tweets_clean_4\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.75, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 4\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 7\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
